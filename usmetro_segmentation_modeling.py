# -*- coding: utf-8 -*-
"""USMetro_Segmentation_Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13znENGO7VS6dFq6rKzelNR-kBPaYppE7
"""

import csv
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score

from google.colab import drive
drive.mount('/content/drive')

pd.set_option('display.max_columns', 500)

pd.options.mode.chained_assignment = None

"""### Data Scaling/Normalization"""

READ_PATH = '/content/drive/My Drive/data/metro_area_features_econ_demog_housing.csv'

econ_demog_data = pd.read_csv(READ_PATH)

econ_demog_data.shape

econ_demog_data.head(5)

econ_demog_data.columns

# Select numeric features
features = ['LaborForceTotal', 
            'Employed',
            'Employed_Fem',
            'Unemployed',
            'HH_Income_200k',
            'Family_Income_200k',
            'Public_Transport',
            'Industry_Manufacturing',
            'Industry_Tech',
            'Industry_Finance',
            'Industry_Healthcare',
            'WFH',
            'Commute_Time',
            'Median_HH_Income',
            'Median_Family_Income',
            'TotalPopulation',
            'Black_Proportion',
            'OtherRace_Proportion',
            'Asian_Proportion',
            'NativeAmerican_Proportion',
            'White_Proportion',
            'MultipleRaces_Proportion',
            'median_sale_price',
            'median_sale_price_yoy',
            'median_ppsf',
            'months_of_supply',
            'months_of_supply_yoy',
            'sold_above_list',
            'off_market_in_two_weeks']
          
econ_demog_features = econ_demog_data[features]

econ_demog_features.shape

scaler = MinMaxScaler()
econ_demog_features_scaled = scaler.fit_transform(econ_demog_features)

econ_demog_features_scaled = pd.DataFrame(econ_demog_features_scaled, columns=econ_demog_features.columns)

econ_demog_features_scaled

econ_demog_features_scaled.shape

"""### Segmentation"""

def km_experiments(df, orig_df, kmin, kmax):
  
  inertia = {}
  silh_scores = {}
  centroids = {}

  for k in range(kmin, kmax+1):
    km = KMeans(n_clusters=k, n_init=20, random_state=1)
    km.fit(df)

    varname = 'Cluster_' + str(k)

    silh = silhouette_score(df, km.labels_)

    inertia[k] = km.inertia_

    silh_scores[k] = silh

    centroids[k] = km.cluster_centers_

    orig_df[varname] = km.labels_

  return (inertia, silh_scores, centroids)

inertia, silh_scores, centroids = km_experiments(econ_demog_features_scaled,
                                                 econ_demog_features,
                                                 kmin=2,
                                                 kmax=15)

silh_scores

inertia

centroids[6].shape

econ_demog_features_scaled.sample(10)



"""### Analyze Segmentation Results

#### Find optimal number of clusters
"""

# Inertia plot
def plot_inertia(inertia_dict):
  fig, ax = plt.subplots(1, 1, figsize=(8, 5))

  x = list(inertia_dict.keys())
  y = list(inertia_dict.values())

  plt.plot(x, y)
  plt.vlines(6, ymin = min(y), ymax=max(y), linestyle=':', color='green')

  plt.title('Inertia (Sum of Squared Errors) by Number of Clusters')
  plt.xlabel('Number of Clusters')
  plt.ylabel('Inertia (Sum of Squared Errors)')
  plt.show()

plot_inertia(intertia)

# Silhouette score plot

def plot_silhouette(silh_dict):
  fig, ax = plt.subplots(1, 1, figsize=(8, 5))

  x = list(silh_dict.keys())
  y = list(silh_dict.values())

  plt.plot(x, y, color='purple')
  plt.vlines(6, ymin = min(y), ymax=max(y), linestyle=':', color='green')
  plt.title('Silhouette Scores by Number of Clusters')
  plt.xlabel('Number of Clusters')
  plt.ylabel('Silhouette Coefficient')
  plt.show()

plot_silhouette(silh_scores)



"""#### Identify Metro Clusters"""

# Cluster statistics
econ_demog_features['Area'] = econ_demog_data['parent_metro_region']

econ_demog_features

econ_demog_features.groupby('Cluster_6').describe(percentiles=[0.5])

# Show metros in each segment

def print_metro_segments(df, cluster_col):
  
  cluster_vals = df[cluster_col].unique()
  cluster_vals.sort()

  for cluster in cluster_vals:
    metros = df.loc[df[cluster_col] == cluster, 'Area']
    print(f'Metros in Group {cluster}: \n {list(metros)}\n')

  return None

print_metro_segments(econ_demog_features, 'Cluster_6')

econ_demog_features[econ_demog_features['Area'] == 'New York, NY']



"""#### Compute Cluster Feature Importance"""

inertia

# Compute feature importances for separation between clusters

def intercluster_feature_importances(k, centroids, scaled_df, raw_df):

  cluster_centroids = centroids[k]

  features = scaled_df.columns

  feature_sse = np.zeros(shape=(k, len(features)))

  for cluster_val in range(k):

    other_clusters = list(set(range(k)) - set([cluster_val]))

    for other_cluster_val in other_clusters:

      curr_centroid = cluster_centroids[cluster_val]
      other_centroid = cluster_centroids[other_cluster_val]

      distance = (curr_centroid - other_centroid)**2

      feature_sse[cluster_val] += distance

  feature_sse = pd.DataFrame(feature_sse, columns=features)

  return(feature_sse)

set([0])

set(range(6))

# Compute feature importances for similarity within clusters

def feature_importances(k, centroids, scaled_df, raw_df):

  cluster_centroids = centroids[k]

  features = scaled_df.columns

  feature_sse = np.zeros(shape=(k, len(features)))

  for cluster_val in range(k):
    metros = scaled_df[raw_df['Cluster_6'] == cluster_val].to_numpy()

    centroid = cluster_centroids[cluster_val]
    
    cluster_feature_sse = np.sum((metros - centroid)**2, axis=0)

    feature_sse[cluster_val] = cluster_feature_sse

  feature_sse = pd.DataFrame(feature_sse, columns=features)

  return(feature_sse)

feat_imp = feature_importances(6,
                               centroids,
                               econ_demog_features_scaled,
                               econ_demog_features)

feat_imp

# Compute total feature importances - smaller values indicate that feature 
# contributed significantly to clustering performance (by minimizing intra-cluster distance)

feat_imp.sum().sort_values()

intercluster_feat_imp = intercluster_feature_importances(6,
                               centroids,
                               econ_demog_features_scaled,
                               econ_demog_features)

intercluster_feat_imp

intercluster_feat_imp.sum().sort_values(ascending=False)

"""### Segmentation with Climate Features"""

READ_PATH = '/content/drive/My Drive/data/metro_area_features.csv'

econ_demog_climate_data = pd.read_csv(READ_PATH)

econ_demog_climate_data.shape

econ_demog_climate_data.sample(5)

econ_demog_climate_data.columns

# Select numeric features
features = ['LaborForceTotal', 
            'Employed',
            'Employed_Fem',
            'Unemployed',
            'HH_Income_200k',
            'Family_Income_200k',
            'Public_Transport',
            'Industry_Manufacturing',
            'Industry_Tech',
            'Industry_Finance',
            'Industry_Healthcare',
            'WFH',
            'Commute_Time',
            'Median_HH_Income',
            'Median_Family_Income',
            'TotalPopulation',
            'Black_Proportion',
            'OtherRace_Proportion',
            'Asian_Proportion',
            'NativeAmerican_Proportion',
            'White_Proportion',
            'MultipleRaces_Proportion',
            'median_sale_price',
            'median_sale_price_yoy',
            'median_ppsf',
            'months_of_supply',
            'months_of_supply_yoy',
            'sold_above_list',
            'off_market_in_two_weeks',
            'DP10',
            'DT32',
            'DX90',
            'PRCP']
          
all_features = econ_demog_climate_data[features]

all_features.sample(5)

scaler = MinMaxScaler()
all_features_scaled = scaler.fit_transform(all_features)

all_features_scaled = pd.DataFrame(all_features_scaled, columns=all_features.columns)

all_features_scaled

inertia, silh_scores, centroids = km_experiments(all_features_scaled,
                                                 all_features,
                                                 kmin=2,
                                                 kmax=15)

inertia

silh_scores

centroids[2].shape



"""#### Climate Segmentation Analysis"""

plot_inertia(inertia)

plot_silhouette(silh_scores)



"""#### Identify Metro Segments"""

all_features

all_features['Area'] = econ_demog_climate_data['parent_metro_region']

all_features.groupby('Cluster_6').describe(percentiles=[0.5])

print_metro_segments(all_features, 'Cluster_6')

"""#### Compute Cluster Feature Importance"""

# Feature importance for intra-cluster similarity
feat_imp = feature_importances(6,
                               centroids,
                               all_features_scaled,
                               all_features)

feat_imp

feat_imp.sum().sort_values()

# Feature importance for inter-cluster separation
intercluster_feat_imp = intercluster_feature_importances(6,
                               centroids,
                               all_features_scaled,
                               all_features)

intercluster_feat_imp

intercluster_feat_imp.sum().sort_values(ascending=False)

